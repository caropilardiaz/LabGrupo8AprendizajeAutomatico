{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 2: Armado de un esquema de aprendizaje automático\n",
    "\n",
    "En el laboratorio final se espera que puedan poner en práctica los conocimientos adquiridos en el curso, trabajando con un conjunto de datos de clasificación.\n",
    "\n",
    "El objetivo es que se introduzcan en el desarrollo de un esquema para hacer tareas de aprendizaje automático: selección de un modelo, ajuste de hiperparámetros y evaluación.\n",
    "\n",
    "El conjunto de datos a utilizar está en `./data/loan_data.csv`. Si abren el archivo verán que al principio (las líneas que empiezan con `#`) describen el conjunto de datos y sus atributos (incluyendo el atributo de etiqueta o clase).\n",
    "\n",
    "Se espera que hagan uso de las herramientas vistas en el curso. Se espera que hagan uso especialmente de las herramientas brindadas por `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Agregar las librerías que hagan falta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model    import SGDClassifier\n",
    "from sklearn.metrics         import accuracy_score\n",
    "from sklearn.metrics         import f1_score\n",
    "from sklearn.metrics         import precision_score\n",
    "from sklearn.metrics         import recall_score\n",
    "from sklearn.metrics         import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree            import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos y división en entrenamiento y evaluación\n",
    "\n",
    "La celda siguiente se encarga de la carga de datos (haciendo uso de pandas). Estos serán los que se trabajarán en el resto del laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"loan_data.csv\", comment=\"#\")\n",
    "\n",
    "# División entre instancias y etiquetas\n",
    "X, y = dataset.iloc[:, 1:], dataset.TARGET\n",
    "\n",
    "# división entre entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Documentación:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Descripción de los Datos y la Tarea\n",
    "\n",
    "Responder las siguientes preguntas:\n",
    "\n",
    "1. ¿De qué se trata el conjunto de datos?\n",
    "2. ¿Cuál es la variable objetivo que hay que predecir? ¿Qué significado tiene?\n",
    "3. ¿Qué información (atributos) hay disponible para hacer la predicción?\n",
    "4. ¿Qué atributos imagina ud. que son los más determinantes para la predicción?\n",
    "\n",
    "**No hace falta escribir código para responder estas preguntas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Es sobre la automizacion del sistema de otogarcion de creditos por parte del banco, \n",
    "   siguendo la Recommendation de Igual de Oportunidad de Acceso a credito.\n",
    "   El modelo de datos se va a entranar con un conjutn ode datos de aplicaciones a creditos recientes.\n",
    "\n",
    "2. La variable objetivo es saber si con lo datos obtenidos la persona sera capaz o no de pagar el credito o se transforma en moroso\n",
    "\n",
    "3. - Loan, es el monto del credito solicitado.\n",
    "   - Mortue, es lo que se debe de la hipoteca aactual\n",
    "   - Value, es el valor actual de la propiedad\n",
    "   - Yoj, la antiguedad en el trabajo actual\n",
    "   - Derog, la cantidad de incedentes relacionados al pago tarde o algun otro porblema con el pago.\n",
    "   - Deling, es cuando no se llega a pagar ni el monto minimo exigido a los 30 dias del vencido el prestamo.\n",
    "   - Clage, es un trackeo de todas las actividades relacionadas con el credito\n",
    "   - Ning, la cantidad de credit lines recientes\n",
    "   - Clno, cantidad total de credit lines\n",
    "   - Debtinc, la relacion en deudas e ingresos\n",
    "\n",
    "\n",
    "4. Diria que para tomar una desicion, las de mas peso serian:\n",
    "   - Loan, el monto solicitado\n",
    "   - Value, valor de la propiedad, si es menor al loan y algo raro hay\n",
    "   - Mortdue, lo que resta de pagar de hipotecas previas.\n",
    "   - Deling, cuantas lineas de credito no llego a pagar en tiempo y forma\n",
    "   - Debtinc, te dice mas o menos como puede responder al credito.\n",
    "   - Yoj, estabilidad laboral\n",
    "   - Y con las demas casi que se podria armar una especie de reputacion creediticia, unificada en solo uan variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Predicción con Modelos Lineales\n",
    "\n",
    "En este ejercicio se entrenarán modelos lineales de clasificación para predecir la variable objetivo.\n",
    "\n",
    "Para ello, deberán utilizar la clase SGDClassifier de scikit-learn.\n",
    "\n",
    "Documentación:\n",
    "- https://scikit-learn.org/stable/modules/sgd.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.1: SGDClassifier con hiperparámetros por defecto\n",
    "\n",
    "Entrenar y evaluar el clasificador SGDClassifier usando los valores por omisión de scikit-learn para todos los parámetros. Únicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
    "\n",
    "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_metrics(y_test, y_pred):\n",
    "    accuracy    = accuracy_score   (y_test, y_pred) \n",
    "    precision   = precision_score  (y_test, y_pred)\n",
    "    recall      = recall_score     (y_test, y_pred)\n",
    "    f1          = f1_score         (y_test, y_pred)\n",
    "    confusion   = confusion_matrix (y_test, y_pred)\n",
    "\n",
    "    \n",
    "    metrics_map = { 'Accuracy'         : accuracy   ,\n",
    "                    'Precision'        : precision  ,\n",
    "                    'Recall'           : recall     , \n",
    "                    'f1_score'         : f1         \n",
    "                  }\n",
    "    \n",
    "    for metric in metrics_map:\n",
    "        print(metric, metrics_map[metric])\n",
    "    \n",
    "    print(confusion)\n",
    "    \n",
    "    print('\\n True Negatives '  + str(confusion[0][0]))\n",
    "    print('\\n False Negatives ' + str(confusion[1][0]))\n",
    "    print('\\n True Positives '  + str(confusion[1][1]))\n",
    "    print('\\n False Positives ' + str(confusion[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8032345013477089\n",
      "Precision 0.16666666666666666\n",
      "Recall 0.03076923076923077\n",
      "f1_score 0.05194805194805195\n",
      "[[296  10]\n",
      " [ 63   2]]\n",
      "\n",
      " True Negatives 296\n",
      "\n",
      " False Negatives 63\n",
      "\n",
      " True Positives 2\n",
      "\n",
      " False Positives 10\n"
     ]
    }
   ],
   "source": [
    "classifier = SGDClassifier(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred     = classifier.predict(X_test)\n",
    "\n",
    "# Look for model evaluation for more info on the metrics\n",
    "print_model_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2: Ajuste de Hiperparámetros\n",
    "\n",
    "Seleccionar valores para los hiperparámetros principales del SGDClassifier. Como mínimo, probar diferentes funciones de loss, tasas de entrenamiento y tasas de regularización.\n",
    "\n",
    "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
    "\n",
    "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
    "\n",
    "Para la mejor configuración encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusión\n",
    "\n",
    "Documentación:\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_map = { 'loss'          : ['hinge','log','squared_hinge','perceptron'],\n",
    "              'penalty'       : ['l2','l1','elasticnet'],\n",
    "              'learning_rate' : ['optimal','adaptive','invscaling', 'adaptive'],\n",
    "              'eta0'          : [0.1, 0.5, 1],\n",
    "              'max_iter'      : [1000, 1500]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier  = SGDClassifier(random_state = 0)\n",
    "grid_search = GridSearchCV(classifier, param_map)\n",
    "kf          = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "# Una aclaracion importante, al principio del lab se dividieron los datos en \n",
    "# train y test, pandas tiene el index, y como los divide random, te quedan mezclados los datos\n",
    "# El kfold se ve que toma el valor de index y lo intenta dividit con eso\n",
    "# Te puede dar indices que no existan en el conjunto de datos parcial de train\n",
    "# pero si en el grupo entero de datos, por eso para este punto nos olvidamos de la division de\n",
    "# datos en train y test, y pasan todos a train y validacion\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_kf, X_val_kf = X.reindex(train_index), X.reindex(val_index)\n",
    "    y_train_kf, y_val_kf = y.reindex(train_index), y.reindex(val_index)\n",
    "    if len(y_train_kf.unique()) > 1: #There has to be more than one class, otherwise it throws an error \n",
    "        grid_search.fit(X_train_kf, y_train_kf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "params  = results['params']\n",
    "mean    = results['mean_test_score']\n",
    "std     = results['std_test_score']\n",
    "print(\"loss\\tpenalty\\tlearning_rate\\teta0\\t| mean\\tstd\\trank\")\n",
    "for p, m, s in zip(params, mean, std):\n",
    "    print(f\"{p['loss']}\\t{p['penalty']}\\t{p['learning_rate']}\\t{p['eta0']}\\t| {m:0.2f}\\t{s:0.2f}\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='squared_hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=0, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8247978436657682\n",
      "Precision 0.0\n",
      "Recall 0.0\n",
      "f1_score 0.0\n",
      "[[306   0]\n",
      " [ 65   0]]\n",
      "\n",
      " True Negatives 306\n",
      "\n",
      " False Negatives 65\n",
      "\n",
      " True Positives 0\n",
      "\n",
      " False Positives 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fer\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "print_model_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Árboles de Decisión\n",
    "\n",
    "En este ejercicio se entrenarán árboles de decisión para predecir la variable objetivo.\n",
    "\n",
    "Para ello, deberán utilizar la clase DecisionTreeClassifier de scikit-learn.\n",
    "\n",
    "Documentación:\n",
    "- https://scikit-learn.org/stable/modules/tree.html\n",
    "  - https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3.1: DecisionTreeClassifier con hiperparámetros por defecto\n",
    "\n",
    "Entrenar y evaluar el clasificador DecisionTreeClassifier usando los valores por omisión de scikit-learn para todos los parámetros. Únicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
    "\n",
    "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9029649595687331\n",
      "Precision 0.7230769230769231\n",
      "Recall 0.7230769230769231\n",
      "f1_score 0.723076923076923\n",
      "[[288  18]\n",
      " [ 18  47]]\n",
      "\n",
      " True Negatives 288\n",
      "\n",
      " False Negatives 18\n",
      "\n",
      " True Positives 47\n",
      "\n",
      " False Positives 18\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state = 0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print_model_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3.2: Ajuste de Hiperparámetros\n",
    "\n",
    "Seleccionar valores para los hiperparámetros principales del DecisionTreeClassifier. Como mínimo, probar diferentes criterios de partición (criterion), profundidad máxima del árbol (max_depth), y cantidad mínima de samples por hoja (min_samples_leaf).\n",
    "\n",
    "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
    "\n",
    "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
    "\n",
    "Para la mejor configuración encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusión\n",
    "\n",
    "\n",
    "Documentación:\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_map = { 'criterion'        : ['gini','entropy'],\n",
    "              'splitter'         : ['best','random'],\n",
    "              'max_depth'        : list(range(2, 31, 5)) + [None],\n",
    "              'min_samples_leaf' : range(1,20, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree        = DecisionTreeClassifier(random_state = 0)\n",
    "grid_search = GridSearchCV(tree, param_map)\n",
    "kf          = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "# Una aclaracion importante, al principio del lab se dividieron los datos en \n",
    "# train y test, pandas tiene el index, y como los divide random, te quedan mezclados los datos\n",
    "# El kfold se ve que toma el valor de index y lo intenta dividit con eso\n",
    "# Te puede dar indices que no existan en el conjunto de datos parcial de train\n",
    "# pero si en el grupo entero de datos, por eso para este punto nos olvidamos de la division de\n",
    "# datos en train y test, y pasan todos a train y validacion\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_kf, X_val_kf = X.reindex(train_index), X.reindex(val_index)\n",
    "    y_train_kf, y_val_kf = y.reindex(train_index), y.reindex(val_index)\n",
    "    if len(y_train_kf.unique()) > 1: #There has to be more than one class, otherwise it throws an error \n",
    "        grid_search.fit(X_train_kf, y_train_kf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion\\splitter\\max_depth\\min_sample_leaf\t| mean\tstd\trank\n",
      "gini\tbest\t2\t1\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t1\t| 0.81\t0.01\t\n",
      "gini\tbest\t2\t3\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t3\t| 0.81\t0.01\t\n",
      "gini\tbest\t2\t5\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t5\t| 0.81\t0.01\t\n",
      "gini\tbest\t2\t7\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t7\t| 0.81\t0.01\t\n",
      "gini\tbest\t2\t9\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t9\t| 0.81\t0.01\t\n",
      "gini\tbest\t2\t11\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t11\t| 0.81\t0.01\t\n",
      "gini\tbest\t2\t13\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t13\t| 0.81\t0.01\t\n",
      "gini\tbest\t2\t15\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t15\t| 0.81\t0.01\t\n",
      "gini\tbest\t2\t17\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t17\t| 0.80\t0.01\t\n",
      "gini\tbest\t2\t19\t| 0.84\t0.02\t\n",
      "gini\trandom\t2\t19\t| 0.80\t0.01\t\n",
      "gini\tbest\t7\t1\t| 0.83\t0.02\t\n",
      "gini\trandom\t7\t1\t| 0.83\t0.01\t\n",
      "gini\tbest\t7\t3\t| 0.83\t0.02\t\n",
      "gini\trandom\t7\t3\t| 0.82\t0.01\t\n",
      "gini\tbest\t7\t5\t| 0.84\t0.02\t\n",
      "gini\trandom\t7\t5\t| 0.82\t0.02\t\n",
      "gini\tbest\t7\t7\t| 0.85\t0.01\t\n",
      "gini\trandom\t7\t7\t| 0.82\t0.02\t\n",
      "gini\tbest\t7\t9\t| 0.85\t0.02\t\n",
      "gini\trandom\t7\t9\t| 0.80\t0.03\t\n",
      "gini\tbest\t7\t11\t| 0.84\t0.02\t\n",
      "gini\trandom\t7\t11\t| 0.81\t0.02\t\n",
      "gini\tbest\t7\t13\t| 0.84\t0.02\t\n",
      "gini\trandom\t7\t13\t| 0.80\t0.02\t\n",
      "gini\tbest\t7\t15\t| 0.84\t0.02\t\n",
      "gini\trandom\t7\t15\t| 0.80\t0.01\t\n",
      "gini\tbest\t7\t17\t| 0.83\t0.02\t\n",
      "gini\trandom\t7\t17\t| 0.79\t0.02\t\n",
      "gini\tbest\t7\t19\t| 0.83\t0.02\t\n",
      "gini\trandom\t7\t19\t| 0.80\t0.03\t\n",
      "gini\tbest\t12\t1\t| 0.82\t0.02\t\n",
      "gini\trandom\t12\t1\t| 0.83\t0.02\t\n",
      "gini\tbest\t12\t3\t| 0.82\t0.02\t\n",
      "gini\trandom\t12\t3\t| 0.83\t0.03\t\n",
      "gini\tbest\t12\t5\t| 0.83\t0.03\t\n",
      "gini\trandom\t12\t5\t| 0.83\t0.03\t\n",
      "gini\tbest\t12\t7\t| 0.83\t0.01\t\n",
      "gini\trandom\t12\t7\t| 0.83\t0.02\t\n",
      "gini\tbest\t12\t9\t| 0.83\t0.02\t\n",
      "gini\trandom\t12\t9\t| 0.82\t0.02\t\n",
      "gini\tbest\t12\t11\t| 0.83\t0.02\t\n",
      "gini\trandom\t12\t11\t| 0.82\t0.01\t\n",
      "gini\tbest\t12\t13\t| 0.83\t0.02\t\n",
      "gini\trandom\t12\t13\t| 0.81\t0.01\t\n",
      "gini\tbest\t12\t15\t| 0.82\t0.01\t\n",
      "gini\trandom\t12\t15\t| 0.80\t0.02\t\n",
      "gini\tbest\t12\t17\t| 0.83\t0.02\t\n",
      "gini\trandom\t12\t17\t| 0.80\t0.02\t\n",
      "gini\tbest\t12\t19\t| 0.83\t0.02\t\n",
      "gini\trandom\t12\t19\t| 0.79\t0.02\t\n",
      "gini\tbest\t17\t1\t| 0.82\t0.02\t\n",
      "gini\trandom\t17\t1\t| 0.83\t0.02\t\n",
      "gini\tbest\t17\t3\t| 0.81\t0.01\t\n",
      "gini\trandom\t17\t3\t| 0.82\t0.02\t\n",
      "gini\tbest\t17\t5\t| 0.82\t0.03\t\n",
      "gini\trandom\t17\t5\t| 0.83\t0.03\t\n",
      "gini\tbest\t17\t7\t| 0.83\t0.01\t\n",
      "gini\trandom\t17\t7\t| 0.83\t0.02\t\n",
      "gini\tbest\t17\t9\t| 0.83\t0.02\t\n",
      "gini\trandom\t17\t9\t| 0.81\t0.03\t\n",
      "gini\tbest\t17\t11\t| 0.83\t0.02\t\n",
      "gini\trandom\t17\t11\t| 0.80\t0.02\t\n",
      "gini\tbest\t17\t13\t| 0.83\t0.01\t\n",
      "gini\trandom\t17\t13\t| 0.80\t0.01\t\n",
      "gini\tbest\t17\t15\t| 0.82\t0.01\t\n",
      "gini\trandom\t17\t15\t| 0.80\t0.02\t\n",
      "gini\tbest\t17\t17\t| 0.82\t0.02\t\n",
      "gini\trandom\t17\t17\t| 0.80\t0.02\t\n",
      "gini\tbest\t17\t19\t| 0.82\t0.02\t\n",
      "gini\trandom\t17\t19\t| 0.80\t0.02\t\n",
      "gini\tbest\t22\t1\t| 0.82\t0.03\t\n",
      "gini\trandom\t22\t1\t| 0.82\t0.02\t\n",
      "gini\tbest\t22\t3\t| 0.81\t0.01\t\n",
      "gini\trandom\t22\t3\t| 0.82\t0.03\t\n",
      "gini\tbest\t22\t5\t| 0.82\t0.03\t\n",
      "gini\trandom\t22\t5\t| 0.81\t0.02\t\n",
      "gini\tbest\t22\t7\t| 0.83\t0.01\t\n",
      "gini\trandom\t22\t7\t| 0.82\t0.02\t\n",
      "gini\tbest\t22\t9\t| 0.83\t0.02\t\n",
      "gini\trandom\t22\t9\t| 0.81\t0.03\t\n",
      "gini\tbest\t22\t11\t| 0.83\t0.02\t\n",
      "gini\trandom\t22\t11\t| 0.82\t0.02\t\n",
      "gini\tbest\t22\t13\t| 0.83\t0.01\t\n",
      "gini\trandom\t22\t13\t| 0.80\t0.01\t\n",
      "gini\tbest\t22\t15\t| 0.82\t0.01\t\n",
      "gini\trandom\t22\t15\t| 0.81\t0.02\t\n",
      "gini\tbest\t22\t17\t| 0.82\t0.02\t\n",
      "gini\trandom\t22\t17\t| 0.80\t0.02\t\n",
      "gini\tbest\t22\t19\t| 0.82\t0.02\t\n",
      "gini\trandom\t22\t19\t| 0.80\t0.02\t\n",
      "gini\tbest\t27\t1\t| 0.82\t0.03\t\n",
      "gini\trandom\t27\t1\t| 0.80\t0.02\t\n",
      "gini\tbest\t27\t3\t| 0.81\t0.01\t\n",
      "gini\trandom\t27\t3\t| 0.82\t0.02\t\n",
      "gini\tbest\t27\t5\t| 0.82\t0.03\t\n",
      "gini\trandom\t27\t5\t| 0.81\t0.02\t\n",
      "gini\tbest\t27\t7\t| 0.83\t0.01\t\n",
      "gini\trandom\t27\t7\t| 0.82\t0.02\t\n",
      "gini\tbest\t27\t9\t| 0.83\t0.02\t\n",
      "gini\trandom\t27\t9\t| 0.81\t0.03\t\n",
      "gini\tbest\t27\t11\t| 0.83\t0.02\t\n",
      "gini\trandom\t27\t11\t| 0.82\t0.02\t\n",
      "gini\tbest\t27\t13\t| 0.83\t0.01\t\n",
      "gini\trandom\t27\t13\t| 0.80\t0.01\t\n",
      "gini\tbest\t27\t15\t| 0.82\t0.01\t\n",
      "gini\trandom\t27\t15\t| 0.81\t0.02\t\n",
      "gini\tbest\t27\t17\t| 0.82\t0.02\t\n",
      "gini\trandom\t27\t17\t| 0.80\t0.02\t\n",
      "gini\tbest\t27\t19\t| 0.82\t0.02\t\n",
      "gini\trandom\t27\t19\t| 0.80\t0.02\t\n",
      "gini\tbest\tNone\t1\t| 0.82\t0.03\t\n",
      "gini\trandom\tNone\t1\t| 0.80\t0.02\t\n",
      "gini\tbest\tNone\t3\t| 0.81\t0.01\t\n",
      "gini\trandom\tNone\t3\t| 0.82\t0.02\t\n",
      "gini\tbest\tNone\t5\t| 0.82\t0.03\t\n",
      "gini\trandom\tNone\t5\t| 0.81\t0.02\t\n",
      "gini\tbest\tNone\t7\t| 0.83\t0.01\t\n",
      "gini\trandom\tNone\t7\t| 0.82\t0.02\t\n",
      "gini\tbest\tNone\t9\t| 0.83\t0.02\t\n",
      "gini\trandom\tNone\t9\t| 0.81\t0.03\t\n",
      "gini\tbest\tNone\t11\t| 0.83\t0.02\t\n",
      "gini\trandom\tNone\t11\t| 0.82\t0.02\t\n",
      "gini\tbest\tNone\t13\t| 0.83\t0.01\t\n",
      "gini\trandom\tNone\t13\t| 0.80\t0.01\t\n",
      "gini\tbest\tNone\t15\t| 0.82\t0.01\t\n",
      "gini\trandom\tNone\t15\t| 0.81\t0.02\t\n",
      "gini\tbest\tNone\t17\t| 0.82\t0.02\t\n",
      "gini\trandom\tNone\t17\t| 0.80\t0.02\t\n",
      "gini\tbest\tNone\t19\t| 0.82\t0.02\t\n",
      "gini\trandom\tNone\t19\t| 0.80\t0.02\t\n",
      "entropy\tbest\t2\t1\t| 0.83\t0.01\t\n",
      "entropy\trandom\t2\t1\t| 0.81\t0.01\t\n",
      "entropy\tbest\t2\t3\t| 0.83\t0.01\t\n",
      "entropy\trandom\t2\t3\t| 0.81\t0.01\t\n",
      "entropy\tbest\t2\t5\t| 0.83\t0.01\t\n",
      "entropy\trandom\t2\t5\t| 0.81\t0.01\t\n",
      "entropy\tbest\t2\t7\t| 0.83\t0.02\t\n",
      "entropy\trandom\t2\t7\t| 0.81\t0.01\t\n",
      "entropy\tbest\t2\t9\t| 0.83\t0.02\t\n",
      "entropy\trandom\t2\t9\t| 0.81\t0.01\t\n",
      "entropy\tbest\t2\t11\t| 0.83\t0.01\t\n",
      "entropy\trandom\t2\t11\t| 0.81\t0.01\t\n",
      "entropy\tbest\t2\t13\t| 0.83\t0.01\t\n",
      "entropy\trandom\t2\t13\t| 0.81\t0.01\t\n",
      "entropy\tbest\t2\t15\t| 0.83\t0.01\t\n",
      "entropy\trandom\t2\t15\t| 0.81\t0.01\t\n",
      "entropy\tbest\t2\t17\t| 0.83\t0.01\t\n",
      "entropy\trandom\t2\t17\t| 0.80\t0.01\t\n",
      "entropy\tbest\t2\t19\t| 0.83\t0.01\t\n",
      "entropy\trandom\t2\t19\t| 0.80\t0.01\t\n",
      "entropy\tbest\t7\t1\t| 0.82\t0.02\t\n",
      "entropy\trandom\t7\t1\t| 0.83\t0.02\t\n",
      "entropy\tbest\t7\t3\t| 0.81\t0.02\t\n",
      "entropy\trandom\t7\t3\t| 0.82\t0.01\t\n",
      "entropy\tbest\t7\t5\t| 0.82\t0.02\t\n",
      "entropy\trandom\t7\t5\t| 0.82\t0.02\t\n",
      "entropy\tbest\t7\t7\t| 0.82\t0.02\t\n",
      "entropy\trandom\t7\t7\t| 0.82\t0.02\t\n",
      "entropy\tbest\t7\t9\t| 0.82\t0.01\t\n",
      "entropy\trandom\t7\t9\t| 0.81\t0.03\t\n",
      "entropy\tbest\t7\t11\t| 0.80\t0.01\t\n",
      "entropy\trandom\t7\t11\t| 0.81\t0.02\t\n",
      "entropy\tbest\t7\t13\t| 0.81\t0.01\t\n",
      "entropy\trandom\t7\t13\t| 0.81\t0.02\t\n",
      "entropy\tbest\t7\t15\t| 0.81\t0.01\t\n",
      "entropy\trandom\t7\t15\t| 0.81\t0.01\t\n",
      "entropy\tbest\t7\t17\t| 0.81\t0.02\t\n",
      "entropy\trandom\t7\t17\t| 0.79\t0.02\t\n",
      "entropy\tbest\t7\t19\t| 0.81\t0.01\t\n",
      "entropy\trandom\t7\t19\t| 0.80\t0.02\t\n",
      "entropy\tbest\t12\t1\t| 0.80\t0.02\t\n",
      "entropy\trandom\t12\t1\t| 0.83\t0.02\t\n",
      "entropy\tbest\t12\t3\t| 0.79\t0.01\t\n",
      "entropy\trandom\t12\t3\t| 0.82\t0.02\t\n",
      "entropy\tbest\t12\t5\t| 0.79\t0.02\t\n",
      "entropy\trandom\t12\t5\t| 0.83\t0.01\t\n",
      "entropy\tbest\t12\t7\t| 0.80\t0.01\t\n",
      "entropy\trandom\t12\t7\t| 0.82\t0.02\t\n",
      "entropy\tbest\t12\t9\t| 0.80\t0.01\t\n",
      "entropy\trandom\t12\t9\t| 0.83\t0.03\t\n",
      "entropy\tbest\t12\t11\t| 0.80\t0.01\t\n",
      "entropy\trandom\t12\t11\t| 0.81\t0.02\t\n",
      "entropy\tbest\t12\t13\t| 0.80\t0.02\t\n",
      "entropy\trandom\t12\t13\t| 0.80\t0.00\t\n",
      "entropy\tbest\t12\t15\t| 0.81\t0.03\t\n",
      "entropy\trandom\t12\t15\t| 0.81\t0.01\t\n",
      "entropy\tbest\t12\t17\t| 0.81\t0.02\t\n",
      "entropy\trandom\t12\t17\t| 0.80\t0.02\t\n",
      "entropy\tbest\t12\t19\t| 0.81\t0.03\t\n",
      "entropy\trandom\t12\t19\t| 0.80\t0.02\t\n",
      "entropy\tbest\t17\t1\t| 0.80\t0.02\t\n",
      "entropy\trandom\t17\t1\t| 0.82\t0.02\t\n",
      "entropy\tbest\t17\t3\t| 0.80\t0.01\t\n",
      "entropy\trandom\t17\t3\t| 0.80\t0.03\t\n",
      "entropy\tbest\t17\t5\t| 0.80\t0.02\t\n",
      "entropy\trandom\t17\t5\t| 0.80\t0.02\t\n",
      "entropy\tbest\t17\t7\t| 0.80\t0.01\t\n",
      "entropy\trandom\t17\t7\t| 0.82\t0.02\t\n",
      "entropy\tbest\t17\t9\t| 0.80\t0.01\t\n",
      "entropy\trandom\t17\t9\t| 0.81\t0.03\t\n",
      "entropy\tbest\t17\t11\t| 0.80\t0.01\t\n",
      "entropy\trandom\t17\t11\t| 0.81\t0.02\t\n",
      "entropy\tbest\t17\t13\t| 0.80\t0.02\t\n",
      "entropy\trandom\t17\t13\t| 0.80\t0.01\t\n",
      "entropy\tbest\t17\t15\t| 0.81\t0.03\t\n",
      "entropy\trandom\t17\t15\t| 0.81\t0.01\t\n",
      "entropy\tbest\t17\t17\t| 0.81\t0.02\t\n",
      "entropy\trandom\t17\t17\t| 0.81\t0.02\t\n",
      "entropy\tbest\t17\t19\t| 0.81\t0.03\t\n",
      "entropy\trandom\t17\t19\t| 0.79\t0.02\t\n",
      "entropy\tbest\t22\t1\t| 0.80\t0.02\t\n",
      "entropy\trandom\t22\t1\t| 0.82\t0.01\t\n",
      "entropy\tbest\t22\t3\t| 0.79\t0.01\t\n",
      "entropy\trandom\t22\t3\t| 0.81\t0.03\t\n",
      "entropy\tbest\t22\t5\t| 0.80\t0.02\t\n",
      "entropy\trandom\t22\t5\t| 0.81\t0.01\t\n",
      "entropy\tbest\t22\t7\t| 0.80\t0.01\t\n",
      "entropy\trandom\t22\t7\t| 0.82\t0.02\t\n",
      "entropy\tbest\t22\t9\t| 0.80\t0.01\t\n",
      "entropy\trandom\t22\t9\t| 0.81\t0.02\t\n",
      "entropy\tbest\t22\t11\t| 0.80\t0.01\t\n",
      "entropy\trandom\t22\t11\t| 0.82\t0.02\t\n",
      "entropy\tbest\t22\t13\t| 0.80\t0.02\t\n",
      "entropy\trandom\t22\t13\t| 0.81\t0.01\t\n",
      "entropy\tbest\t22\t15\t| 0.81\t0.03\t\n",
      "entropy\trandom\t22\t15\t| 0.80\t0.02\t\n",
      "entropy\tbest\t22\t17\t| 0.81\t0.02\t\n",
      "entropy\trandom\t22\t17\t| 0.81\t0.02\t\n",
      "entropy\tbest\t22\t19\t| 0.81\t0.03\t\n",
      "entropy\trandom\t22\t19\t| 0.80\t0.02\t\n",
      "entropy\tbest\t27\t1\t| 0.80\t0.02\t\n",
      "entropy\trandom\t27\t1\t| 0.81\t0.03\t\n",
      "entropy\tbest\t27\t3\t| 0.79\t0.01\t\n",
      "entropy\trandom\t27\t3\t| 0.83\t0.02\t\n",
      "entropy\tbest\t27\t5\t| 0.80\t0.02\t\n",
      "entropy\trandom\t27\t5\t| 0.81\t0.01\t\n",
      "entropy\tbest\t27\t7\t| 0.80\t0.01\t\n",
      "entropy\trandom\t27\t7\t| 0.83\t0.01\t\n",
      "entropy\tbest\t27\t9\t| 0.80\t0.01\t\n",
      "entropy\trandom\t27\t9\t| 0.81\t0.02\t\n",
      "entropy\tbest\t27\t11\t| 0.80\t0.01\t\n",
      "entropy\trandom\t27\t11\t| 0.82\t0.02\t\n",
      "entropy\tbest\t27\t13\t| 0.80\t0.02\t\n",
      "entropy\trandom\t27\t13\t| 0.81\t0.01\t\n",
      "entropy\tbest\t27\t15\t| 0.81\t0.03\t\n",
      "entropy\trandom\t27\t15\t| 0.80\t0.02\t\n",
      "entropy\tbest\t27\t17\t| 0.81\t0.02\t\n",
      "entropy\trandom\t27\t17\t| 0.81\t0.02\t\n",
      "entropy\tbest\t27\t19\t| 0.81\t0.03\t\n",
      "entropy\trandom\t27\t19\t| 0.80\t0.02\t\n",
      "entropy\tbest\tNone\t1\t| 0.80\t0.02\t\n",
      "entropy\trandom\tNone\t1\t| 0.81\t0.02\t\n",
      "entropy\tbest\tNone\t3\t| 0.79\t0.01\t\n",
      "entropy\trandom\tNone\t3\t| 0.82\t0.02\t\n",
      "entropy\tbest\tNone\t5\t| 0.80\t0.02\t\n",
      "entropy\trandom\tNone\t5\t| 0.81\t0.01\t\n",
      "entropy\tbest\tNone\t7\t| 0.80\t0.01\t\n",
      "entropy\trandom\tNone\t7\t| 0.83\t0.01\t\n",
      "entropy\tbest\tNone\t9\t| 0.80\t0.01\t\n",
      "entropy\trandom\tNone\t9\t| 0.80\t0.03\t\n",
      "entropy\tbest\tNone\t11\t| 0.80\t0.01\t\n",
      "entropy\trandom\tNone\t11\t| 0.82\t0.02\t\n",
      "entropy\tbest\tNone\t13\t| 0.80\t0.02\t\n",
      "entropy\trandom\tNone\t13\t| 0.81\t0.01\t\n",
      "entropy\tbest\tNone\t15\t| 0.81\t0.03\t\n",
      "entropy\trandom\tNone\t15\t| 0.80\t0.02\t\n",
      "entropy\tbest\tNone\t17\t| 0.81\t0.02\t\n",
      "entropy\trandom\tNone\t17\t| 0.81\t0.02\t\n",
      "entropy\tbest\tNone\t19\t| 0.81\t0.03\t\n",
      "entropy\trandom\tNone\t19\t| 0.80\t0.02\t\n"
     ]
    }
   ],
   "source": [
    "results = grid_search.cv_results_\n",
    "params  = results['params']\n",
    "mean    = results['mean_test_score']\n",
    "std     = results['std_test_score']\n",
    "print(\"criterion\\splitter\\max_depth\\min_sample_leaf\\t| mean\\tstd\\trank\")\n",
    "for p, m, s in zip(params, mean, std):\n",
    "    print(f\"{p['criterion']}\\t{p['splitter']}\\t{p['max_depth']}\\t{p['min_samples_leaf']}\\t| {m:0.2f}\\t{s:0.2f}\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=7, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=7, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8814016172506739\n",
      "Precision 0.7058823529411765\n",
      "Recall 0.5538461538461539\n",
      "f1_score 0.6206896551724139\n",
      "[[291  15]\n",
      " [ 29  36]]\n",
      "\n",
      " True Negatives 291\n",
      "\n",
      " False Negatives 29\n",
      "\n",
      " True Positives 36\n",
      "\n",
      " False Positives 15\n"
     ]
    }
   ],
   "source": [
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "print_model_metrics(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
